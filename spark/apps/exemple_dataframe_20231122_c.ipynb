{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 02: Introduction to DataFrame (according to spark)\n",
    "\n",
    "\n",
    "## Quick guide to using DataFrame\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "python api that lists all the functions applicable to a dataframe.\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html\n",
    "\n",
    "The aim of this tutorial is to start a spark session and manipulate dataframes by applying basic processing.\n",
    "\n",
    "1. Start a pyspark session\n",
    "\n",
    "2. Create a dataframe from an RDD or a list (don't forget to define a structure).\n",
    "\n",
    "3. Create a pandas dataframe from a dataframe (spark)\n",
    "\n",
    "4. Perform operations (compare with list processing)\n",
    "\n",
    "5. Grouping and joins\n",
    "\n",
    "6. Reading a csv\n",
    " \n",
    " https://notebooks.gesis.org/binder/jupyter/user/apache-spark-awl6064c/notebooks/python/docs/source/getting_started/quickstart_df.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python package list\n",
    "# !python --version\n",
    "# !pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_aarch64.whl.metadata (3.2 kB)\n",
      "Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_aarch64.whl (45.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-22.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start a spark session\n",
    "\n",
    "(we want to start a local session and give it a name).\n",
    "\n",
    "We'll use the SparkSession class from the [*pyspark.sql*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html) sub-module .\n",
    "\n",
    "Call *sc* the *spark.sparkContext*.\n",
    "\n",
    "- *builder*: A class attribute having a Builder to construct SparkSession instances\n",
    "\n",
    "- Specify what is defined by [*SparkSession*](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html) and *sparkContext*.\n",
    "\n",
    "- the master is 'local' (*master(\"local[\\*])* (it is possible to specify the number of cores *local[\\*]* or for example *local[4]*) \n",
    "\n",
    "- the application must be given a name *appName('name appli')*.\n",
    "\n",
    "- *getOrCreate()* : Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "\n",
    "# import pyspark and sparksession\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# link pyspark and spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define spark session\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").appName('TP02_dataframe').getOrCreate()\n",
    "# create sparkContext variable\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://019f6fb2a5ee:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TP02_dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=TP02_dataframe>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pypsark version == spark version\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A port (4040 by default) that you should definitely look at (link Spark UI)\n",
    "\n",
    "It allows you to follow the progress of a spark process (Spark UI).\n",
    "\n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[*.parallelize()*](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.parallelize.html) : Distribute a local Python collection to form an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a dataframe\n",
    "\n",
    "### 2.1 Create a dataframe from rdd\n",
    "- create RDD from a list\n",
    "     - create list (list_1 and list_2)\n",
    "     - create rdd (rdd_l1 from list_1 and rdd_l2 from list_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ParallelCollectionRDD[75] at readRDDFromFile at PythonRDD.scala:297\n",
      "[(1, 0), (1, 8), (1, 3), (1, 3), (1, 1), (1, 9), (1, 6), (1, 6), (1, 11), (1, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# list\n",
    "list_1 = [(1, 0), (1, 8), (1, 3), (1, 3), (1, 1), (1, 9), (1, 6), (1, 6), (1, 11), (1, 2)]\n",
    "print(type(list_1))\n",
    "\n",
    "# Build RDD\n",
    "rdd_l1 = sc.parallelize(list_1, 2)\n",
    "print(rdd_l1)\n",
    "print(rdd_l1.collect())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "ParallelCollectionRDD[76] at readRDDFromFile at PythonRDD.scala:297\n",
      "[{'numero': 1, 'valeur': 0}, {'numero': 1, 'valeur': 2}]\n"
     ]
    }
   ],
   "source": [
    "list_2 = [{'numero': 1, 'valeur': 0},{'numero': 1, 'valeur': 2}]\n",
    "\n",
    "print(type(list_2))\n",
    "\n",
    "# Build RDD\n",
    "rdd_l2 = sc.parallelize(list_2, 2)\n",
    "print(rdd_l2)\n",
    "print(rdd_l2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create an RDD (resp. dl ) using the parallelize function of the sparkContext and the 2 previously created lists (list_1 and l1).\n",
    "- display the number of partitions using the getNumPartitions() function (from the RDD)`\n",
    "\n",
    "- Dataframe : \n",
    "\n",
    "[pysparl.sql.DataFrame]( https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html#)\n",
    "\n",
    "\n",
    "- Create a DataFrame \n",
    "\n",
    "[pyspark.sql.SparkSession.createDataFrame](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  0|\n",
      "|  1|  8|\n",
      "|  1|  3|\n",
      "|  1|  3|\n",
      "|  1|  1|\n",
      "|  1|  9|\n",
      "|  1|  6|\n",
      "|  1|  6|\n",
      "|  1| 11|\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "== Physical Plan ==\n",
      "* Scan ExistingRDD (1)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [2]: [_1#525L, _2#526L]\n",
      "Arguments: [_1#525L, _2#526L], MapPartitionsRDD[81] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe from a RDD \n",
    "df = spark.createDataFrame(list_1)\n",
    "\n",
    "# Display the dataframe (only a small one or a small part)\n",
    "df.show()\n",
    "\n",
    "# Display the schema\n",
    "df.printSchema()\n",
    "\n",
    "df.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| v2|value|\n",
      "+---+-----+\n",
      "|  1|    0|\n",
      "|  1|    8|\n",
      "|  1|    3|\n",
      "|  1|    3|\n",
      "|  1|    1|\n",
      "|  1|    9|\n",
      "|  1|    6|\n",
      "|  1|    6|\n",
      "|  1|   11|\n",
      "|  1|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It's better with a column names\n",
    "df = spark.createDataFrame(rdd_l1,['v2', 'value'])\n",
    "\n",
    "# Display dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- v3: integer (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| v3|value|\n",
      "+---+-----+\n",
      "|  1|    0|\n",
      "|  1|    8|\n",
      "|  1|    3|\n",
      "|  1|    3|\n",
      "|  1|    1|\n",
      "|  1|    9|\n",
      "|  1|    6|\n",
      "|  1|    6|\n",
      "|  1|   11|\n",
      "|  1|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a schema (StructType)\n",
    "\n",
    "# dl1 has two columns\n",
    "schema =    StructType([\n",
    "        StructField(\"v3\", IntegerType(), True),\n",
    "        StructField(\"value\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "# use the previous rdd (dl1) to create dataframe df1\n",
    "df1 = spark.createDataFrame(rdd_l1,schema)\n",
    "\n",
    "# display schema\n",
    "df1.printSchema()\n",
    "\n",
    "# display result\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numero: long (nullable = true)\n",
      " |-- valeur: long (nullable = true)\n",
      "\n",
      "+------+------+\n",
      "|numero|valeur|\n",
      "+------+------+\n",
      "|     1|     0|\n",
      "|     1|     2|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# again with rdd_l2\n",
    "df = spark.createDataFrame(rdd_l2)\n",
    "\n",
    "# display schema\n",
    "df.printSchema()\n",
    "\n",
    "# display result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the elements number\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the elements numlber where value is > 5  in df (utiliser where et count)\n",
    "df.where('valeur > 5').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|numero|valeur|\n",
      "+------+------+\n",
      "|     1|     0|\n",
      "|     1|     2|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "      <th>valeur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numero  valeur\n",
       "0       1       0\n",
       "1       1       2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show()\n",
    "panda = df.toPandas()\n",
    "\n",
    "panda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a panda dataframe\n",
    "\n",
    "- Creating a spark dataframe from a panda dataframe\n",
    "\n",
    "- Display the dataframe and its schema\n",
    "\n",
    "We will use the list indicated in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.show()\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "panda_df\n",
      "   a    b        c           d                   e\n",
      "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
      "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
      "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00\n",
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [a#580L, b#581, c#582, d#583, e#584], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "a: bigint, b: double, c: string, d: date, e: timestamp\n",
      "LogicalRDD [a#580L, b#581, c#582, d#583, e#584], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [a#580L, b#581, c#582, d#583, e#584], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[a#580L,b#581,c#582,d#583,e#584]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"df.show()\")\n",
    "\n",
    "df.show()\n",
    "print(\"panda_df\")\n",
    "\n",
    "print(pandas_df)\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "-RECORD 1------------------\n",
      " a   | 2                   \n",
      " b   | 3.0                 \n",
      " c   | string2             \n",
      " d   | 2000-02-01          \n",
      " e   | 2000-01-02 12:00:00 \n",
      "-RECORD 2------------------\n",
      " a   | 3                   \n",
      " b   | 4.0                 \n",
      " c   | string3             \n",
      " d   | 2000-03-01          \n",
      " e   | 2000-01-03 12:00:00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display i columns (but it's possible in row)\n",
    "\n",
    "df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataframe operation\n",
    "\n",
    "- select columns\n",
    "\n",
    "- add new columns by processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a dataframe (panda) consisting of 4 columns (1st column of type int, 2nd of type string, 3rd column a date, 4th column a float value)\n",
    "\n",
    "- name the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+-----+------------------+\n",
      "|summary|            indice| prenom|  nom|            valeur|\n",
      "+-------+------------------+-------+-----+------------------+\n",
      "|  count|                 4|      4|    4|                 4|\n",
      "|   mean|               2.5|   NULL| NULL|               2.5|\n",
      "| stddev|1.2909944487358056|   NULL| NULL|1.2909944487358056|\n",
      "|    min|                 1|jacques|  doe|               1.0|\n",
      "|    max|                 4| pierre|smith|               4.0|\n",
      "+-------+------------------+-------+-----+------------------+\n",
      "\n",
      "+------+-------+------+----------+------+\n",
      "|indice| prenom|   nom|      date|valeur|\n",
      "+------+-------+------+----------+------+\n",
      "|     1|   john|   doe|2000-01-01|   1.0|\n",
      "|     2|   jean|dupond|2000-02-01|   2.0|\n",
      "|     3| pierre| smith|2000-03-01|   3.0|\n",
      "|     4|jacques|durand|2000-04-01|   4.0|\n",
      "+------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'indice': [1, 2, 3, 4],\n",
    "    'prenom': ['john', 'jean', 'pierre', 'jacques'],\n",
    "    'nom': ['doe', 'dupond', 'smith', 'durand'],\n",
    "    'date': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1), date(2000, 4, 1)],\n",
    "    'valeur': [1., 2., 3., 4.]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "df.describe().show()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add a 5th column indicating the type of data in the 2nd column\n",
    "\n",
    "- add a 6th column which is the result of an arithmetic operation on the 4th column\n",
    "  \n",
    "- display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+------+---------+\n",
      "|indice| prenom|   nom|      date|valeur|operation|\n",
      "+------+-------+------+----------+------+---------+\n",
      "|     1|   john|   doe|2000-01-01|   1.0|      4.0|\n",
      "|     2|   jean|dupond|2000-02-01|   2.0|      5.0|\n",
      "|     3| pierre| smith|2000-03-01|   3.0|      6.0|\n",
      "|     4|jacques|durand|2000-04-01|   4.0|      7.0|\n",
      "+------+-------+------+----------+------+---------+\n",
      "\n",
      "+------+-------+------+----------+------+--------------+\n",
      "|indice| prenom|   nom|      date|valeur|    nom+prenom|\n",
      "+------+-------+------+----------+------+--------------+\n",
      "|     1|   john|   doe|2000-01-01|   1.0|      john doe|\n",
      "|     2|   jean|dupond|2000-02-01|   2.0|   jean dupond|\n",
      "|     3| pierre| smith|2000-03-01|   3.0|  pierre smith|\n",
      "|     4|jacques|durand|2000-04-01|   4.0|jacques durand|\n",
      "+------+-------+------+----------+------+--------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [indice#617L, prenom#618, nom#619, date#620, valeur#621], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "indice: bigint, prenom: string, nom: string, date: date, valeur: double\n",
      "LogicalRDD [indice#617L, prenom#618, nom#619, date#620, valeur#621], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [indice#617L, prenom#618, nom#619, date#620, valeur#621], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[indice#617L,prenom#618,nom#619,date#620,valeur#621]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"operation\",df.valeur +3).show()\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "df.withColumn(\"nom+prenom\", concat_ws(\" \",\"prenom\",\"nom\")).show()\n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apply an operation on the column using a function\n",
    "\n",
    "(pandas functions can be applied)\n",
    "https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/02 13:55:49 WARN TaskSetManager: Lost task 0.0 in stage 81.0 (TID 176) (172.18.0.4 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 2028, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1938, in read_udfs\n",
      "    read_single_udf(\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 911, in read_single_udf\n",
      "    return wrap_scalar_pandas_udf(func, args_offsets, kwargs_offsets, return_type, runner_conf)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in wrap_scalar_pandas_udf\n",
      "    arrow_return_type = to_arrow_type(\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/types.py\", line 93, in to_arrow_type\n",
      "    import pyarrow as pa\n",
      "ModuleNotFoundError: No module named 'pyarrow'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:117)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/12/02 13:55:50 ERROR TaskSetManager: Task 0 in stage 81.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 2028, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1938, in read_udfs\n    read_single_udf(\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 911, in read_single_udf\n    return wrap_scalar_pandas_udf(func, args_offsets, kwargs_offsets, return_type, runner_conf)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in wrap_scalar_pandas_udf\n    arrow_return_type = to_arrow_type(\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/types.py\", line 93, in to_arrow_type\n    import pyarrow as pa\nModuleNotFoundError: No module named 'pyarrow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m multiply \u001b[38;5;241m=\u001b[39m pandas_udf(multiply_func, returnType\u001b[38;5;241m=\u001b[39mFloatType())\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Execute function as a Spark vectorized UDF\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvaleur\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvaleur\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mexplain(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformatted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtripled_col\u001b[39m\u001b[38;5;124m'\u001b[39m, tripled(df\u001b[38;5;241m.\u001b[39mvaleur))\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/classic/dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/classic/dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 2028, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1938, in read_udfs\n    read_single_udf(\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 911, in read_single_udf\n    return wrap_scalar_pandas_udf(func, args_offsets, kwargs_offsets, return_type, runner_conf)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 123, in wrap_scalar_pandas_udf\n    arrow_return_type = to_arrow_type(\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/types.py\", line 93, in to_arrow_type\n    import pyarrow as pa\nModuleNotFoundError: No module named 'pyarrow'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/02 13:57:02 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.4: worker lost: 172.18.0.4:7000 got disassociated\n",
      "25/12/02 13:57:02 ERROR TaskSchedulerImpl: Lost executor 2 on 172.18.0.3: worker lost: 172.18.0.3:7000 got disassociated\n",
      "25/12/02 13:57:02 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.5: worker lost: 172.18.0.5:7000 got disassociated\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "\n",
    "@udf(\"float\") \n",
    "def tripled(num):\n",
    "  return 3*float(num)\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=FloatType())\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\"*\",multiply(col(\"valeur\"), col(\"valeur\"))).show()\n",
    "df.explain(mode=\"formatted\")\n",
    "df.withColumn('tripled_col', tripled(df.valeur)).show()\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. The return of grouping, but applied to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following dataframe\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- group by color and calculate average value and total number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupBy(\"color\",\"fruit\").mean(\"v1\",\"v2\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge and add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two following dataframes:\n",
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    ('time', 'id', 'v1'))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
    "    ('time', 'id', 'v2'))\n",
    "\n",
    "# use inner, left, right,...\n",
    "\n",
    "df1.join(df2,df1.time ==  df2.time,\"inner\") \\\n",
    "     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a grouping to create a 4-column dataframe (time id v1 v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two time columns are identical\n",
    "df1.join(df2,[\"time\"],\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV file and create dataframe\n",
    "\n",
    "- read csv file and display dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# read the file trafficaerien.csv\n",
    "\n",
    "dftext = spark.read.csv(\"data/trafficaerien.csv\")\n",
    "dftext.printSchema()\n",
    "\n",
    "dftext.show()\n",
    "\n",
    "dftext1 = spark.read.options(delimiter=';').option(\"header\",True).option(\"dateFormat\", \"yyyy-MM\").csv(\"data/trafficaerien.csv\")\n",
    "dftext1.printSchema()\n",
    "\n",
    "dftext1 = dftext1.withColumn('Month', to_date(dftext1['Month'], 'yyyy-MM'))\n",
    "\n",
    "dftext1.printSchema()\n",
    "\n",
    "dftext1.show()\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField(\"Month\", DateType(), True),\n",
    "    StructField(\"Passengers\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "dftext2 = spark.read.options(delimiter=';').option(\"header\",True)\\\n",
    "   .option(\"dateFormat\", \"yyyy-MM\").option(\"inferSchema\", \"true\")\\\n",
    "   .csv(\"data/trafficaerien.csv\", schema = mySchema)\n",
    "\n",
    "\n",
    "dftext2.printSchema()\n",
    "\n",
    "dftext2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the spark session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
